{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTbCHpG+NTcFC2RHvId9EC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LimaCondas/eco-driving-speed-rl/blob/main/My_Gym_simple.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Description of Eco-Driving Environment .ipynb File\n",
        "This file contains a basic implementation of the Eco-Driving gym environment, using Python and the OpenAI Gym library. **The environment is designed to simulate a car driving scenario on a straight, flat road with no air friction or inner cost.**\n",
        "\n",
        "The car is controlled through acceleration and deacceleration actions, and the traction force is assumed to be fully converted into acceleration of the vehicle. **The reward system considers the traction work and the speed deviation from the safety speed.**\n",
        "\n",
        "The EcoDrivingEnv class is defined and initialized in the notebook, providing the basic structure for the environment. A test environment is created and run with random actions to verify the environment is working as expected.\n",
        "\n",
        "A deep learning model is built using the Keras library, which is then used to create an agent to interact with the environment. The keras-RL library is used to train and optimize the agent, using the deep learning model and the EcoDrivingEnv environment.\n",
        "\n",
        "Overall, this notebook provides a simple but effective starting point for experimenting with Eco-Driving environments and reinforcement learning. This file was inspired by a YouTube video https://youtu.be/bD6V3rcr_54, which provided guidance on implementing the Eco-Driving environment using OpenAI Gym. "
      ],
      "metadata": {
        "id": "nbqihGtpOF0k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**0.Install Dependencies**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ZsHKnip8uaR-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9tIlbQ7tXg6"
      },
      "outputs": [],
      "source": [
        "!pip install gym\n",
        "!pip install numpy==1.20\n",
        "!pip install tensorflow==2.5.0\n",
        "!pip install keras\n",
        "!pip install keras-rl2\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **1. Test a Simple Eco-Driving Scenario**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FmU-E2Ysk-7U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **import packages**"
      ],
      "metadata": {
        "id": "oFkFjNiqlIMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gym import Env\n",
        "from gym.spaces import Discrete, Box\n",
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "ZFqX2BABwjsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **This block is EcoDrivingEnv**"
      ],
      "metadata": {
        "id": "Geq5S6bW6qCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class EcoDrivingEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        self.safe_speed_limit = 20 # m/s, equal 72kph\n",
        "        self.time_step = 0.1 # seconds\n",
        "        self.mass = 1000 # kg\n",
        "        self.gravity = 9.81 # m/s^2\n",
        "        self.MAX_SPEED = 40 # m/s, equal 144kph\n",
        "        self.traction_coefficient = 0.8\n",
        "        self.position = 0\n",
        "        self.speed = 20\n",
        "        self.work = 0\n",
        "        self.steps = 0\n",
        "        self.total_reward = 0\n",
        "        self.distance = 1000 # m\n",
        "        self.max_steps = 5000 # 5000 steps, 500s\n",
        "        self.max_acceleration = 1 # m/s^2\n",
        "\n",
        "        # Action Space deaccelerate, no action, accelerate\n",
        "        self.action_space = spaces.Discrete(3) \n",
        "\n",
        "        # Observation space [position, speed]\n",
        "        self.observation_space = spaces.Box(low=np.array([0, 0]), high=np.array([np.inf, np.inf]), dtype=np.float32)\n",
        "\n",
        "        print(\"Successfully Initialize EcoDrivingEnv.......\")\n",
        "\n",
        "    def reset(self):\n",
        "        self.position = 0\n",
        "        self.speed = 0\n",
        "        self.work = 0\n",
        "        self.steps = 0\n",
        "        self.total_reward = 0\n",
        "        return np.array([self.position, self.speed], dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        # Apply action\n",
        "        if action == 0:\n",
        "            acceleration = -self.max_acceleration # -1\n",
        "        elif action == 1:\n",
        "            acceleration = 0\n",
        "        elif action == 2:\n",
        "            acceleration = self.max_acceleration # +1\n",
        "        else:\n",
        "            acceleration = 0\n",
        "\n",
        "\n",
        "        # Calculate new speed and position\n",
        "        new_speed = self.speed + acceleration * self.time_step # vt = v0 + a * dt\n",
        "        new_speed = np.clip(new_speed, 0, self.MAX_SPEED) # speed no bigger than max speed\n",
        "\n",
        "        # xt = x0 + vt + 0.5 * a * t^2\n",
        "        new_position = self.position + self.speed * self.time_step + 0.5 * acceleration * self.time_step ** 2\n",
        "\n",
        "        # Calculate work of traction force\n",
        "        traction_force = self.traction_coefficient * self.mass * acceleration # F = 0.8 * m * a\n",
        "        self.work += traction_force * (new_position - self.position) # W = F * dx\n",
        "\n",
        "        # Calculate speed deviation from safe speed limit\n",
        "        speed_deviation = abs(new_speed - self.safe_speed_limit)\n",
        "\n",
        "        # Calculate Reward\n",
        "        # reward = - 0.0015 * self.work  - 0.9985 * speed_deviation \n",
        "        reward = - 0.001 * self.work / (self.work + speed_deviation) - 0.999 * speed_deviation / (self.work + speed_deviation)\n",
        "        # print('Work: {}\\t  speed_deviation:{}'.format(self.work, speed_deviation))\n",
        "\n",
        "        # Update state\n",
        "        self.position = new_position\n",
        "        self.speed = new_speed\n",
        "        self.steps += 1\n",
        "\n",
        "        # Check if episode is done, distance or steps satisfied\n",
        "        done = False\n",
        "        if self.position >= self.distance or self.steps >= self.max_steps:\n",
        "          done = True\n",
        "\n",
        "        # Update info dictionary with additional information\n",
        "        self.state = np.array([self.position, self.speed], dtype=np.float32)\n",
        "        info = {'friction_work': self.work, 'speed_deviation': speed_deviation}\n",
        "\n",
        "        # Return step information\n",
        "        return self.state, reward, done, info\n",
        "\n",
        "\n",
        "    def render(self):\n",
        "        print(f\"Position: {self.position:.2f}m, Speed: {self.speed:.2f}m/s, Work: {self.work:.2f}J, Steps: {self.steps}\")\n",
        "        pass"
      ],
      "metadata": {
        "id": "-64sYdR2yUz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Examplify an Eco-Drving Env**"
      ],
      "metadata": {
        "id": "_vyFNGRsi-nO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = EcoDrivingEnv()"
      ],
      "metadata": {
        "id": "LcbwFLuBUOb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "849617ab-ef79-4695-9e09-470fd99c2f94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully Initialize EcoDrivingEnv.......\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Test Environment with Random Action**"
      ],
      "metadata": {
        "id": "Mme9Yvk3lgJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# env.action_space.sample()\n",
        "# env.observation_space.sample()"
      ],
      "metadata": {
        "id": "drQyPYzpUTdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "episode = 10\n",
        "\n",
        "flag = 0\n",
        "position = []\n",
        "speed = []\n",
        "\n",
        "for episode in range(1, episode+1):\n",
        "  state = env.reset()\n",
        "  done = False\n",
        "  score = 0\n",
        "  \n",
        "  while not done:\n",
        "    action = random.choice([0, 1, 2])\n",
        "    n_state, reward, done, info = env.step(action)\n",
        "    score += reward\n",
        "\n",
        "    if not flag:\n",
        "      position.append(n_state[0])\n",
        "      speed.append(n_state[1])\n",
        "  flag = 1\n",
        "\n",
        "  print('===== Episode:{} Score:{} ====='.format(episode, int(score)))\n"
      ],
      "metadata": {
        "id": "pB2XhTNeU_8Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7f60a6a-fe85-40ac-efcc-8606b73d37e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Episode:1 Score:-134 =====\n",
            "===== Episode:2 Score:-205 =====\n",
            "===== Episode:3 Score:-158 =====\n",
            "===== Episode:4 Score:-196 =====\n",
            "===== Episode:5 Score:-60 =====\n",
            "===== Episode:6 Score:-145 =====\n",
            "===== Episode:7 Score:-247 =====\n",
            "===== Episode:8 Score:-324 =====\n",
            "===== Episode:9 Score:-195 =====\n",
            "===== Episode:10 Score:-154 =====\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **SAMPLE to plot the profile of position and speed in 1 episode**"
      ],
      "metadata": {
        "id": "J8eGKQuKiuTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "\n",
        "# position_data = position\n",
        "# speed_data = speed\n",
        "\n",
        "# # Create a figure with 2 subplots for position and speed\n",
        "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))\n",
        "# fig.subplots_adjust(hspace=0.4)\n",
        "\n",
        "# # Plot position data\n",
        "# ax1.plot(position_data, '-')\n",
        "# ax1.set_title('Profile of Position')\n",
        "# ax1.set_xlabel('Time (step)')\n",
        "# ax1.set_ylabel('Position')\n",
        "\n",
        "# # Plot speed data\n",
        "# ax2.plot(speed_data, '-')\n",
        "# ax2.set_title('Profile of Speed')\n",
        "# ax2.set_xlabel('Time (step)')\n",
        "# ax2.set_ylabel('Speed')\n",
        "\n",
        "# # Display the plot\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "CQetOD6zh4sP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Create a Deep Learning Model with Keras**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Z0d58r4666bH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import deque\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "puqyZvqKnE_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "states = env.observation_space.shape\n",
        "actions = env.action_space.n"
      ],
      "metadata": {
        "id": "FPW0WsG4nXAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "states"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aULZoyCmouCT",
        "outputId": "516beda8-add6-4094-92e5-c463dc58886d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2,)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(states, actions):\n",
        "  model = Sequential()\n",
        "  model.add(Flatten(input_shape=(1,) + states))  # Add a Flatten layer\n",
        "  model.add(Dense(24, activation='relu', input_shape=states))\n",
        "  model.add(Dense(24, activation='relu'))\n",
        "  model.add(Dense(actions, activation='linear'))\n",
        "  return model"
      ],
      "metadata": {
        "id": "QYcKLJU7pZow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del model # if error, delete this"
      ],
      "metadata": {
        "id": "e5FGwy3znkkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(states, actions)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQaNYhpRiKZg",
        "outputId": "9fe667d2-1684-410b-97e4-d33a351c3173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_4 (Flatten)          (None, 2)                 0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 24)                72        \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 24)                600       \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 3)                 75        \n",
            "=================================================================\n",
            "Total params: 747\n",
            "Trainable params: 747\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Build Agent with keras-RL**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n_n001pxnJpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rl.agents import DQNAgent\n",
        "from rl.policy import BoltzmannQPolicy\n",
        "from rl.memory import SequentialMemory"
      ],
      "metadata": {
        "id": "7xEyMo4InRgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_agent(model, actions):\n",
        "  policy = BoltzmannQPolicy()\n",
        "  memory = SequentialMemory(limit=50000, window_length=1)\n",
        "  dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
        "                 nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
        "  return dqn"
      ],
      "metadata": {
        "id": "m9HltD3LnoFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dqn = build_agent(model, actions)\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
      ],
      "metadata": {
        "id": "_wtb6uzcnHpm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5467d6e1-90bd-4627-94b3-d9a4c874f8f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 50000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 99s 10ms/step - reward: -0.0208\n",
            "6 episodes - episode_reward: -21.067 [-43.640, -9.897] - loss: 0.049 - mae: 4.282 - mean_q: 3.509 - friction_work: 19556.410 - speed_deviation: 13.833\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 90s 9ms/step - reward: -0.0120\n",
            "4 episodes - episode_reward: -48.040 [-123.526, -18.641] - loss: 0.014 - mae: 2.048 - mean_q: 2.739 - friction_work: 9919.367 - speed_deviation: 15.493\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 88s 9ms/step - reward: -0.0256\n",
            "4 episodes - episode_reward: -23.121 [-39.249, -9.945] - loss: 0.009 - mae: 1.586 - mean_q: 2.403 - friction_work: 9536.994 - speed_deviation: 15.680\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 91s 9ms/step - reward: -0.0155\n",
            "4 episodes - episode_reward: -68.065 [-172.956, -14.678] - loss: 0.008 - mae: 1.526 - mean_q: 2.315 - friction_work: 6697.541 - speed_deviation: 16.481\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 92s 9ms/step - reward: -0.0433\n",
            "done, took 460.535 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5fe63fc370>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    }
  ]
}